{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e9192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################\n",
    "##### Notebook Processamento de Linguagem natural (PLN)\n",
    "##### Baseado em:\n",
    "## Natural Language Processing with Python (book)\n",
    "##\n",
    "##############################################################################################################\n",
    "## Objetivos:\n",
    "##   Mostrar varios metodos de linguagem natural utilizando Python\n",
    "###################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d53311",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "### 01 - Tokenizacao\n",
    "################################################\n",
    "\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "text = \"This is Mary's car, isn't it?\"\n",
    "tk_list = []\n",
    "tk_list.append(WhitespaceTokenizer()) \n",
    "tk_list.append(WordPunctTokenizer())\n",
    "tk_list.append(TreebankWordTokenizer())\n",
    "\n",
    "for tk in tk_list:\n",
    "    result = tk.tokenize(text) \n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725d3320",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenizacao em portugues\n",
    "\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "\n",
    "text = \"guarda-chuva Se a única coisa que de o homem terá certeza é a morte; a única certeza do brasileiro é o carnaval no próximo ano.\" # Graciliano Ramos\n",
    "\n",
    "result = tokenize.word_tokenize(text, language='portuguese') \n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0d2b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "### 02 - Stemming\n",
    "################################################\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "example_words = [\"program\",\"programming\",\"programer\",\"programs\",\"programmed\"]\n",
    "\n",
    "# Perform stemming\n",
    "print(\"{0:20}{1:20}\".format(\"--Word--\",\"--Stem--\"))\n",
    "for word in example_words:\n",
    "   print (\"{0:20}{1:20}\".format(word, ps.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5335ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_words = [\"programmers\", \"because\", \"people\"]\n",
    "print(\"{0:20}{1:20}\".format(\"--Word--\",\"--Stem--\"))\n",
    "for word in example_words:\n",
    "   print (\"{0:20}{1:20}\".format(word, ps.stem(word)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20880b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "### 03 - Lemmanization\n",
    "################################################\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "words = ['dogs', 'churches', 'aardwolves', 'abaci', 'hardrock']\n",
    "\n",
    "for w in words:\n",
    "    print(wnl.lemmatize(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0a76ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "### 04 - Stopwords\n",
    "################################################\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "print(stopwords[:15])\n",
    "len(stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e003ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "### 04 - Caracterizacao das palavras - Tfidf\n",
    "################################################\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "texts = [\"bad movie\", \"not a good movie\", \"did not like\", \"i like it\", \"good me\"]\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1 ,2))\n",
    "features = tfidf.fit_transform(texts)\n",
    "df = pd.DataFrame(\n",
    "    features.todense(),\n",
    "    columns=tfidf.get_feature_names_out()\n",
    ")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d3532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "### Extra - Lembrando como fazer hashing\n",
    "################################################\n",
    "\n",
    "import hashlib\n",
    "\n",
    "def hash_token(token, b):\n",
    "    hash_object = hashlib.sha256()\n",
    "    hash_object.update(token.encode()) # UTF-8 encode\n",
    "    return int(hash_object.hexdigest(), 16) % (2**b)\n",
    "\n",
    "# Example usage\n",
    "b = 10  # Number of buckets for hashing\n",
    "token = \"dfadfasdfasdfasdfadfadfadfasdfasdfasdfa\"\n",
    "hashed_value = hash_token(token, b)\n",
    "print(hashed_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2c776c",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "### 05 - Caracterizacao das palavras - wordvector\n",
    "################################################\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sentencas\n",
    "sentences = [[\"gato\", \"persegue\", \"rato\"], [\"cachorro\", \"late\", \"muito\"], [\"lobo\", \"uiva\"]]\n",
    "\n",
    "# Treinamento do modelo Word2Vec\n",
    "model = Word2Vec(sentences, min_count=1) # ignora palavras com frequencia abaixo de...\n",
    "\n",
    "vector = model.wv['gato']\n",
    "print(\"Vetor representando 'gato':\", vector)\n",
    "\n",
    "# Find similar words\n",
    "similar_words = model.wv.most_similar('gato')\n",
    "print(\"Similaridade das palavras em relacao a 'gato 'gato':\", similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444fa978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"I like to eat apples\",\n",
    "    \"I like bananas\",\n",
    "    \"I enjoy eating oranges\"\n",
    "]\n",
    "\n",
    "# Tokenize the corpus\n",
    "tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in corpus]\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# Get the word vector for a word\n",
    "word_vector = model.wv[\"apples\"]\n",
    "print(\"Vector for 'apples':\", word_vector)\n",
    "\n",
    "# Find similar words\n",
    "similar_words = model.wv.most_similar(\"apples\")\n",
    "print(\"Similar words to 'apples':\", similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964d1b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################################\n",
    "##################################################################################################################\n",
    "##################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da5972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData():\n",
    "    data = ['This is a  dog','This is a cat','I love my cat','This is my name ']\n",
    "    dat=[]\n",
    "    for i in range(len(data)):\n",
    "        for word in data[i].split():\n",
    "            dat.append(word)\n",
    "    print(dat)\n",
    "    return dat\n",
    "\n",
    "def createBigram(data):\n",
    "   listOfBigrams = []\n",
    "   bigramCounts = {}\n",
    "   unigramCounts = {}\n",
    "   for i in range(len(data)-1):\n",
    "      if i < len(data) - 1 and data[i+1].islower():\n",
    "\n",
    "         listOfBigrams.append((data[i], data[i + 1]))\n",
    "\n",
    "         if (data[i], data[i+1]) in bigramCounts:\n",
    "            bigramCounts[(data[i], data[i + 1])] += 1\n",
    "         else:\n",
    "            bigramCounts[(data[i], data[i + 1])] = 1\n",
    "\n",
    "      if data[i] in unigramCounts:\n",
    "         unigramCounts[data[i]] += 1\n",
    "      else:\n",
    "         unigramCounts[data[i]] = 1\n",
    "   return listOfBigrams, unigramCounts, bigramCounts\n",
    "\n",
    "\n",
    "def calcBigramProb(listOfBigrams, unigramCounts, bigramCounts):\n",
    "    listOfProb = {}\n",
    "    for bigram in listOfBigrams:\n",
    "        word1 = bigram[0]\n",
    "        word2 = bigram[1]\n",
    "        listOfProb[bigram] = (bigramCounts.get(bigram))/(unigramCounts.get(word1))\n",
    "    return listOfProb\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = readData()\n",
    "    listOfBigrams, unigramCounts, bigramCounts = createBigram(data)\n",
    "\n",
    "    print(\"\\n All the possible Bigrams are \")\n",
    "    print(listOfBigrams)\n",
    "\n",
    "    print(\"\\n Bigrams along with their frequency \")\n",
    "    print(bigramCounts)\n",
    "\n",
    "    print(\"\\n Unigrams along with their frequency \")\n",
    "    print(unigramCounts)\n",
    "\n",
    "    bigramProb = calcBigramProb(listOfBigrams, unigramCounts, bigramCounts)\n",
    "\n",
    "    print(\"\\n Bigrams along with their probability \")\n",
    "    print(bigramProb)\n",
    "    inputList=\"This is my cat\"\n",
    "    splt=inputList.split()\n",
    "    outputProb1 = 1\n",
    "    bilist=[]\n",
    "    bigrm=[]\n",
    "\n",
    "    for i in range(len(splt) - 1):\n",
    "        if i < len(splt) - 1:\n",
    "\n",
    "            bilist.append((splt[i], splt[i + 1]))\n",
    "\n",
    "    print(\"\\n The bigrams in given sentence are \")\n",
    "    print(bilist)\n",
    "    for i in range(len(bilist)):\n",
    "        if bilist[i] in bigramProb:\n",
    "\n",
    "            outputProb1 *= bigramProb[bilist[i]]\n",
    "        else:\n",
    "\n",
    "            outputProb1 *= 0\n",
    "    print('\\n' + 'Probablility of sentence \\\"This is my cat\\\" = ' + str(outputProb1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d23c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from nltk import word_tokenize\n",
    "\n",
    "folder = 'C:/Users/dealbuqc/Desktop/UFPB/Classes/SBC/'\n",
    "teste_tagger = joblib.load(folder+'POS_tagger_bigram2.pkl')\n",
    "phrase = 'O rato roeu a roupa do rei de Roma'\n",
    "teste_tagger.tag(word_tokenize(phrase))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a15d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "nltk.download('mac_morpho')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ff9b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = list(nltk.corpus.mac_morpho.tagged_sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a1f261",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199f76b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot = len(dataset)\n",
    "tot_train_samples = int(np.ceil(tot*.8))\n",
    "\n",
    "train_data = dataset[:tot_train_samples]\n",
    "test_data = dataset[tot_train_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102fffe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_def = nltk.DefaultTagger('N')\n",
    "t_affix2 = nltk.AffixTagger(train_data, affix_length=-2, backoff=t_def)\n",
    "t_affix3 = nltk.AffixTagger(train_data, affix_length=-3, backoff=t_affix2)\n",
    "t_affix4 = nltk.AffixTagger(train_data, affix_length=-4, backoff=t_affix3)\n",
    "t_affix5 = nltk.AffixTagger(train_data, affix_length=-5, backoff=t_affix4)\n",
    "t_affix6 = nltk.AffixTagger(train_data, affix_length=-6, backoff=t_affix5)\n",
    "\n",
    "acc_def = t_def.evaluate(test_data) * 100\n",
    "acc_af2 = t_affix2.evaluate(test_data) * 100\n",
    "acc_af3 = t_affix3.evaluate(test_data) * 100\n",
    "acc_af4 = t_affix4.evaluate(test_data) * 100\n",
    "acc_af5 = t_affix5.evaluate(test_data) * 100\n",
    "acc_af6 = t_affix6.evaluate(test_data) * 100\n",
    "\n",
    "print('''Performance dos taggers:\n",
    "         - Default:                     {:.2f}%\n",
    "         - Sufixo tamanho 2 + Default:  {:.2f}%\n",
    "         - Sufixo tamanho 3 + Sufixo 2: {:.2f}%\n",
    "         - Sufixo tamanho 4 + Sufixo 3: {:.2f}%\n",
    "         - Sufixo tamanho 5 + Sufixo 4: {:.2f}%\n",
    "         - Sufixo tamanho 6 + Sufixo 5: {:.2f}%'''.format(acc_def, acc_af2, acc_af3,\n",
    "                                                          acc_af4, acc_af5, acc_af6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0954d189",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_def = nltk.DefaultTagger('N')\n",
    "t_affix2 = nltk.AffixTagger(train_data, affix_length=-2, backoff=t_def)\n",
    "t_affix3 = nltk.AffixTagger(train_data, affix_length=-3, backoff=t_affix2)\n",
    "t_affix4 = nltk.AffixTagger(train_data, affix_length=-4, backoff=t_affix3)\n",
    "t_affix5 = nltk.AffixTagger(train_data, affix_length=-5, backoff=t_affix4)\n",
    "t_affix6 = nltk.AffixTagger(train_data, affix_length=-6, backoff=t_affix5)\n",
    "\n",
    "acc_def = t_def.evaluate(test_data) * 100\n",
    "acc_af2 = t_affix2.evaluate(test_data) * 100\n",
    "acc_af3 = t_affix3.evaluate(test_data) * 100\n",
    "acc_af4 = t_affix4.evaluate(test_data) * 100\n",
    "acc_af5 = t_affix5.evaluate(test_data) * 100\n",
    "acc_af6 = t_affix6.evaluate(test_data) * 100\n",
    "\n",
    "print('''Performance dos taggers:\n",
    "         - Default:                     {:.2f}%\n",
    "         - Sufixo tamanho 2 + Default:  {:.2f}%\n",
    "         - Sufixo tamanho 3 + Sufixo 2: {:.2f}%\n",
    "         - Sufixo tamanho 4 + Sufixo 3: {:.2f}%\n",
    "         - Sufixo tamanho 5 + Sufixo 4: {:.2f}%\n",
    "         - Sufixo tamanho 6 + Sufixo 5: {:.2f}%'''.format(acc_def, acc_af2, acc_af3,\n",
    "                                                          acc_af4, acc_af5, acc_af6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fbad47",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_uni = nltk.UnigramTagger(train_data, backoff=t_affix5)\n",
    "\n",
    "acc_uni = t_uni.evaluate(test_data) * 100\n",
    "\n",
    "print('''Performance dos taggers:\n",
    "         - Default:                     {:.2f}%\n",
    "         - Sufixo tamanho 2 + Default:  {:.2f}%\n",
    "         - Sufixo tamanho 3 + Sufixo 2: {:.2f}%\n",
    "         - Sufixo tamanho 4 + Sufixo 3: {:.2f}%\n",
    "         - Sufixo tamanho 5 + Sufixo 4: {:.2f}%\n",
    "         - Sufixo tamanho 6 + Sufixo 5: {:.2f}%\n",
    "         - Unigrama + Sufixo 6:         {:.2f}%'''.format(acc_def, acc_af2, acc_af3,\n",
    "                                                          acc_af4, acc_af5, acc_af6,\n",
    "                                                          acc_uni))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab303543",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_bi = nltk.BigramTagger(train_data, backoff=t_uni)\n",
    "t_tri = nltk.TrigramTagger(train_data, backoff=t_bi)\n",
    "\n",
    "acc_bi = t_bi.evaluate(test_data) * 100\n",
    "acc_tri = t_tri.evaluate(test_data) * 100\n",
    "\n",
    "print('''Performance dos taggers:\n",
    "         - Default:                     {:.2f}%\n",
    "         - Sufixo tamanho 2 + Default:  {:.2f}%\n",
    "         - Sufixo tamanho 3 + Sufixo 2: {:.2f}%\n",
    "         - Sufixo tamanho 4 + Sufixo 3: {:.2f}%\n",
    "         - Sufixo tamanho 5 + Sufixo 4: {:.2f}%\n",
    "         - Sufixo tamanho 6 + Sufixo 5: {:.2f}%\n",
    "         - Unigrama + Sufixo 6:         {:.2f}%\n",
    "         - Bigrama + Unigrama:          {:.2f}%\n",
    "         - Trigrama + Bigrama:          {:.2f}%'''.format(acc_def, acc_af2, acc_af3,\n",
    "                                                          acc_af4, acc_af5, acc_af6,\n",
    "                                                          acc_uni, acc_bi, acc_tri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7705b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(t_bi, open(folder+'POS_tagger_bigram2.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48dc744",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---\n",
    "\n",
    "def viterbi(obs, states, start_p, trans_p, emit_p):\n",
    "    V = [{}]\n",
    "    for st in states:\n",
    "        V[0][st] = {\"prob\": start_p[st] * emit_p[st][obs[0]], \"prev\": None}\n",
    "    for t in range(1, len(obs)):\n",
    "        V.append({})\n",
    "        for st in states:\n",
    "            max_tr_prob = max(V[t-1][prev_st][\"prob\"]*trans_p[prev_st][st] for prev_st in states)\n",
    "            for prev_st in states:\n",
    "                if V[t-1][prev_st][\"prob\"]*trans_p[prev_st][st] == max_tr_prob:\n",
    "                    max_prob = max_tr_prob * emit_p[st][obs[t]]\n",
    "                    V[t][st] = {\"prob\": max_prob, \"prev\": prev_st}\n",
    "                    break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b288f57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ('Rainy', 'Sunny')\n",
    "\n",
    "observations = ('walk', 'shop', 'clean')\n",
    "\n",
    "start_probability = {'Rainy': 0.6, 'Sunny': 0.4}\n",
    "\n",
    "transition_probability = {\n",
    "'Rainy' : {'Rainy': 0.7, 'Sunny': 0.3},\n",
    "'Sunny' : {'Rainy': 0.4, 'Sunny': 0.6},\n",
    "}\n",
    "\n",
    "emission_probability = {\n",
    "'Rainy' : {'walk': 0.1, 'shop': 0.4, 'clean': 0.5},\n",
    "'Sunny' : {'walk': 0.6, 'shop': 0.3, 'clean': 0.1},\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0612c95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "viterbi(observations,\n",
    "         states,\n",
    "         start_probability,\n",
    "         transition_probability,\n",
    "         emission_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0e15aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.utils as ku \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554f523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "data = open('../input/dtspeech/DTSpeech.txt').read()\n",
    "corpus = data.lower().split(\"\\n\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db25a6cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
